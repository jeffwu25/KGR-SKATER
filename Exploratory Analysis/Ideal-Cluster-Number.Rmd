---
title: "Ideal Cluster Number"
author: "Jeffrey Wu"
date: "2023-06-07"
output: pdf_document
---

```{r}
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)
library(readxl)
library(spdep)
library(sp)
library(INLA)
library(HMMpa)
library(invgamma)

soa.data = read_xlsx("SoA.data.1019.xlsx")

###county_flips are unique identifier for counties
soa.data$county_fips = as.character(soa.data$county_fips) ##change it to character

###DON'T DO THIS ANYMORE!!!
###Could ALSO estimate a time series model for each county and cluster on parameter values for each county (another form of dimension reduction)
###Practice on LA county
# LA = soa.data %>% filter(county_name == "Los Angeles County") %>%
#   select(Score,Total_Pop, EDUC_Lessthan9, EDUC_college, White_Collar,
#                                Unemployment_Rate, Adj_HH_income, Income_Disparity,
#                                Individuals_Below_Poverty, Median_Home_Value,
#                                Median_Gross_Rent, Housing_No_Telephone,
#                                Housing_Incomplete_Plumbing)
# test = apply(LA,2,mean)
# 
# for (i in 5:17){
#   CA_newdata[19,i] = test[i-4]
# }

###Boil down each feature's time series (2010-2019) to summary stat eg mean 
CA_newdata = soa.data[1:58,]
CA_newdata = CA_newdata[,-c(4,7,8)]
countynames = CA_newdata$county_name

for (i in 1:58){
  features = soa.data %>% filter(county_name == countynames[i]) %>%
    select(Score,Total_Pop, EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value,
                               Median_Gross_Rent, Housing_No_Telephone,
                               Housing_Incomplete_Plumbing)
  features.avg = apply(features,2,mean)
  for (j in 5:17){
    CA_newdata[i,j] = features.avg[j-4]
  }
}

#IMPORTANT

# This shape file contains the coordinates for county boundaries
##counties is from urbanmap

CA.counties = counties %>% filter(state_abbv == "CA")

CA.counties2 = read.csv("counties.ca.data.csv")
ca.coordinates = data.frame(CA.counties2$county,CA.counties2$lat,CA.counties2$lng)
colnames(ca.coordinates) = c("county","lat","long")

ca.coordinates = ca.coordinates[order(ca.coordinates$county),]
row.names(ca.coordinates) = NULL


###IF WE DON"T WANT TO BOIL DOWN TIME SERIES AND KEEP ALL DATA, SWITCH CA_newdata w soa.data below

soa_joint <- left_join(CA.counties, CA_newdata, by = "county_fips")
lat0=min(soa_joint$lat)
lat1=max(soa_joint$lat)

soa_joint %>%
  ggplot(aes(long, lat, group = group, fill = Score)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(65, 185))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Deprivation Score")) +
  ggtitle(paste0("Deprivation Score in California by County"))+
  xlab("lon") +ylab("lat" )

CA_newdata = cbind(CA_newdata,ca.coordinates[,2:3])

#Use with CA_newdata (aggregated time series data)
CA_data = soa_joint %>% select(long, lat, county_name.y, Score, Total_Pop,
                               EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value,
                               Median_Gross_Rent, Housing_No_Telephone,
                               Housing_Incomplete_Plumbing)

colnames(CA_data)[3] = "County"
```



SKATER clustering: 

```{r}
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

covariates_scale = data.frame(apply(CA_newdata[,7:17],2,scale))
#covariates_scale = data.frame(scale((CA_newdata$EDUC_Lessthan9)))

CA_spdf@data = covariates_scale
```


```{r}
#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#summary(CA_nb)

# plot(CA_spdf, main = "With queen")
# plot(CA_nb, coords = coordinates(CA_spdf), col="blue", add = TRUE)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

# plot(ct_mst,coordinates(CA_spdf),col="blue", cex.lab=0.5)
# plot(CA_spdf, add=TRUE)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

CA_data_cluster = (CA_sf %>% mutate(clus = clus5$groups))

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")
```


Determine optimal number of clusters (between 5-10?): 
DOESN"T WORK FOR RAW TIME SERIES DATA (TOO MUCH MEMORY) SO USE AGGREGATED DATA INSTEAD

```{r}
#Silhouette plots 
#(-1 is prob wrong cluster, 0 is right on cusp could go either way, close to 1 is good)

library(cluster)
library (factoextra)

#Identifies ideal # of clusters 1-10
fviz_nbclust(covariates_scale, FUN = hcut, method = 'silhouette') 

#Disassociation matrix (what should we put in there?)
dis = daisy(covariates_scale,metric = "euclidean")

#Silhouette plot for clusters of 2-10
for (i in 1:9){
  clus.i <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = i)
  plot((CA_sf %>% mutate(clus = clus.i$groups))['clus'], main = "Cluster example no constraints")
  
  sil = silhouette(clus.i$groups,dis)
  plot(sil)
}
```



STEPWISE PROCEDURE (ADDING SUBINDICES ONE BY ONE) TO EVALUATE IF NBCLUST AND SILHOUETTE METHOD AGREE FOR OPTIMAL # CLUSTERS 

COVARIATES SCALED SET: (EDUC_Lessthan9)

NBClust: 9 avg width about 0.6? , 2 is also high

Silhouette plot: largest avg width was 0.24 (2 or 3 clusters), 9 clusters had avg width -0.26 


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college)

NBClust: 9 avg width about 0.5 , 3,10 are also high 

Silhouette plot: largest avg width was 0.4 (2 clusters), basically all others had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar)

NBClust: 3 avg width about 0.45, 2 is also high

Silhouette plot: largest avg width was 0.42 (2 clusters), basically all others had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate)

NBClust: 2 avg width about 0.4, definitively

Silhouette plot: largest avg width was 0.37 (2 clusters), basically all others had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,Adj_HH_income)

NBClust: 2 avg width about 0.4 , definitively

Silhouette plot: largest avg width was 0.38 (2 clusters), clusters 6-10 had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity)

NBClust: 2 avg width about 0.4, 3-5 and 8-10 appear to be just under 0.3

Silhouette plot: largest avg width was 0.5 (2 clusters), clusters 7-10 had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity,Individuals_Below_Poverty)

NBClust: 2 avg width about 0.4, 8-10 appear to be just under 0.3

Silhouette plot: largest avg width was 0.47 (2 clusters), basically all others had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity,Individuals_Below_Poverty,
                        Median_Home_Value)

NBClust: 2 avg width about 0.4, 5-6 slightly larger than 0.3

Silhouette plot: largest avg width was 0.49 (2 clusters), clusters 7-10 had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity,Individuals_Below_Poverty,
                        Median_Home_Value,Median_Gross_Rent)

NBClust: 2 avg width about 0.4, all others less than 0.3

Silhouette plot: largest avg width was 0.48 (2 clusters), clusters 8-10 had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity,Individuals_Below_Poverty,
                        Median_Home_Value,Median_Gross_Rent, Housing_No_Telephone)

NBClust: 2 avg width about 0.35, more noticeable decreasing trend with more clusters now 

Silhouette plot: largest avg width was 0.46 (2 clusters), clusters 7-10 had negative avg width


COVARIATES SCALED SET: (EDUC_Lessthan9,EDU_college,White_Collar,Unemployment_Rate,
                        Adj_HH_income,Income_Disparity,Individuals_Below_Poverty,
                        Median_Home_Value,Median_Gross_Rent, Housing_No_Telephone,
                        Housing_Incomplete_Plumbing)

NBClust: 2 avg width about 0.35, more noticeable decreasing trend with more clusters now 
(slight uptick at the end tho)

Silhouette plot: largest avg width was 0.48 (2 clusters), clusters 7-10 had negative avg width


TAKEAWAYS: 

- Although the avg widths calculated were slightly different, the 2 methods definitely agree that less clusters is better ESPECIALLY as number of features clustered on increases 
- Seems like 2 clusters is way to go (STARTED AGREEING AFTER 4 or more features)
- NbClust was selecting 9 clusters for first few steps (small subset of features), while silhouette method saw these as incorrect and preferred low # clusters all the way through 
- As I plotted the SKATER 5 clusters map for each step, I noticed that with no constraints, it would consistently group most counties together into one big cluster, then one or two for everything else -> supports idea of small # clusters 
- NbClust did hint that 8-10 clusters could be kindaaa appropriate 
- Silhouette plots NEVER liked more than 8 clusters 
- Did not perform formal stepwise selection on subindices, but if I randomly put 3-5 subindices in there the general trends above remain true 



```{r}
#SKATER CLUSTERS 5-10
for (i in 4:9){
  clus.i <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = i)
  plot((CA_sf %>% mutate(clus = clus.i$groups))['clus'], main = "Cluster example with population constraint")
  
  sil = silhouette(clus.i$groups,dis)
  plot(sil)
}
```


When evaluating 5-10 clusters (clustered on Score + ALL subindices), we can see that

For 5: Average width is 0.14, 3 clusters only have 2 counties, 2 clusters have negative average widths 

For 6: Average width is 0.13, 3 clusters only have 2 counties and one only has 1, 2 clusters have negative average widths and one has 0 

For 7: Average width is -0.15, 3 clusters only have 2 counties and one only has 1, 3 clusters have negative average widths and one has 0 

For 8: Average width is -0.3, 3 clusters only have 2 counties and one only has 1, 4 clusters have negative average widths and one has 0 

For 9: Average width is -0.32, 4 clusters only have 2 counties and one only has 1, 4 clusters have negative average widths and one has 0 

For 10: Average width is -0.34, 4 clusters only have 2 counties and 2 clusters only have 1, 4 clusters have negative average widths and two have 0 



```{r}
#SKATER 2 CLUSTERS 
clus2 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 1)
clus2_min <- skater(edges = ct_mst[,1:2], 
                     data = covariates_scale, 
                     crit = 10000000, 
                     vec.crit = CA_data$Total_Pop,
                     ncuts = 1)

plot((CA_sf %>% mutate(clus = clus2$groups))['clus'], main = "2 cluster example")
plot((CA_sf %>% mutate(clus = clus2_min$groups))['clus'], main = "2 cluster example with population constraint")
  
sil = silhouette(clus2$groups,dis)
plot(sil)

sil = silhouette(clus2_min$groups,dis)
plot(sil)
```

FOR 2 clusters (NO POP CONSTRAINT), we get avg silhouette width of 0.14 and a cluster with 52 and one with 6

(WITH POP CONSTRAINT MUCH WORSE)


