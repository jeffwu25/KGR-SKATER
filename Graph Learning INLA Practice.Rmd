---
title: "INLA Practice"
author: "Jeffrey Wu"
date: "2023-04-18"
output: pdf_document
---

SKATER: create contiguous spatial clusters based on covariates or response? 


```{r}
library(sp)
library(spdep)
library(cancensus)
library(dplyr)

options(cancensus.api_key = "CensusMapper_f522d0edee93906c0878389b45338272")

#Toy data from Canada census from cancensus

child_census_vectors("v_CA16_5660") %>% 
  pull(label)

occ_vec <- c("v_CA16_5660", child_census_vectors("v_CA16_5660") %>% 
  pull(vector))

ct_occs <- get_census(dataset='CA16', 
                          regions=list(CSD=c("5915022","5915025","5915029"),
                                       CT=c("9330069.02","9330069.01","9330008.01")), 
                          vectors=occ_vec, labels="short", geo_format='sf', 
                          level='CT', api_key = )

ct_dat <- ct_occs %>% 
  rename(total = v_CA16_5660, occ0 = v_CA16_5663, occ1 = v_CA16_5666, occ2 = v_CA16_5669, 
         occ3 = v_CA16_5672, occ4 = v_CA16_5675, occ5 = v_CA16_5678, occ6 = v_CA16_5681,
         occ7 = v_CA16_5684, occ8 = v_CA16_5687, occ9 = v_CA16_5690) %>% 
  mutate(across(.cols = occ0:occ9,
                .fns = ~./total)) %>% 
  select(GeoUID, occ0:occ9)

glimpse(ct_dat)

plot(ct_dat[,c(2:10)], max.plot = 10)

ct_dat[,5]

#Scale and center values 
ct_scaled <- ct_dat %>% 
  st_drop_geometry() %>% 
  mutate(across(.cols = occ0:occ9,
                .fns = ~scale(.))) 
```


```{r}
#Create adjacency matrix 
ct_nb <- poly2nb(as_Spatial(ct_dat))

costs <- nbcosts(ct_nb, data = ct_scaled[,-1])
ct_w <- nb2listw(ct_nb,costs,style="B")

ct_mst <- mstree(ct_w)

plot(ct_mst,coordinates(as_Spatial(ct_dat)),col="blue", cex.lab=0.5)
plot(as_Spatial(ct_dat), add=TRUE)

#Run SKATER -- 9 partitions so 10 clusters 
clus10 <- skater(edges = ct_mst[,1:2], data = ct_scaled[,-1], ncuts = 9)

clus10$groups

#Evaluate what is the appropriate number of clusters 1-10
#Use Nbclust or silhouette 

plot((ct_dat %>% mutate(clus = clus10$groups))['clus'], main = "10 cluster example")
```


```{r}
#Add population constraints 
ct_occs %>% st_drop_geometry() %>% 
    mutate(clus = clus10$groups) %>% 
    group_by(clus) %>% summarise(pop = sum(Population)) %>% 
    ggplot(., aes(x = factor(clus), y = pop)) + geom_col() + theme_minimal()

# Requiring a minimum population size of 40000 per cluster
clus10_min <- skater(edges = ct_mst[,1:2], 
                     data = ct_scaled[,-1], 
                     crit = 40000, 
                     vec.crit = ct_occs$Population,
                     ncuts = 9)
```


```{r}
#SKATER for poverty in CA 
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)

poverty = read.csv("poverty.csv")

#Convert state and county into factors? 
#poverty$State = factor(poverty$State)
#poverty$County = factor(poverty$County)

#Change variables into proportions 
poverty$Poverty = poverty$Poverty/100
poverty$Men = poverty$Men/poverty$TotalPop
poverty$Women = poverty$Women/poverty$TotalPop

poverty[,7:23] = poverty[,7:23]/100
poverty[,25:33] = poverty[,25:33]/100

###county_flips are unique identifier for counties
poverty$county_fips = as.character(poverty$county_fips) ##change it to character
##delete na flips
missing_fips = which(is.na(poverty$county_fips))
if(length(missing_fips)>0){
  poverty = poverty[-missing_fips,]
}

for (i in 1:dim(poverty)[1]){
  county_fips_iter = poverty$county_fips[i]
  county_fips_length = str_length(county_fips_iter)
  diff = 5-county_fips_length
  if (diff<5){
    poverty$county_fips[i] = paste0(do.call(paste0,as.list((rep(0,diff)))), county_fips_iter)
  }
}

#IMPORTANT
poverty_joint <- left_join(poverty, counties, by = "county_fips")

#Plot county level poverty for California 
# This shape file contains the coordinates for county boundaries
##counties is from urbanmap
state_name='California'
state_shape_data = counties %>%
  filter(state_name == state_name)

##get the specific state
CA_poverty = poverty %>%
  filter(State == "CA")

###IMPORTANT
#counties$county_fips = as.numeric(counties$county_fips)
CA_poverty_joint <- left_join(CA_poverty, counties, by = "county_fips")

lat0=min(CA_poverty_joint$Lat)
lat1=max(CA_poverty_joint$Lat)

CA_poverty_joint %>%
  ggplot(aes(long, lat, group = group, fill = Poverty)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 0.55))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Poverty Rate")) +
  ggtitle(paste0("Poverty Rates in California by County"))+
  xlab("lon") +ylab("lat" )

#CA_data = CA_poverty %>% select(Poverty, Lat, Long)

CA_data = CA_poverty %>% select(County, Poverty, TotalPop, AvgAge, 
                                MeanCommute, Unemployment, Lat, Long)
```

MAKING A TIME SERIES FOR POVERTY DATA: 

```{r}
#Let's say data comes from 2010
#Simulate 2011-2019 data based on 2010
Year = rep(2010,58)
CApoverty.ts = cbind(CA_data,Year)
ts2 = cbind(CA_data,Year)
values = ts2[,c("Poverty","AvgAge","MeanCommute","Unemployment")]

for (j in 1:9){
  
  for (i in 1:nrow(ts2)){
  ts2$Poverty[i] = rnorm(1,values[i,1],0.02)
  ts2$AvgAge[i] = rnorm(1,values[i,2],2)
  ts2$MeanCommute[i] = rnorm(1,values[i,3],2)
  ts2$Unemployment[i] = rnorm(1,values[i,2],0.02)
  ts2$Year[i] = 2010+j
  }

  CApoverty.ts = rbind(CApoverty.ts,ts2)
}

#Boil down time series for each county into summary statistics then do clustering 
```





```{r}
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

poverty_scaled = scale(CA_data$Poverty)
age_scale = scale(CA_data$AvgAge)

covariates_scale = data.frame(poverty_scaled,age_scale)

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

plot(CA_spdf, main = "With queen")
plot(CA_nb, coords = coordinates(CA_spdf), col="blue", add = TRUE)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat()
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

plot(ct_mst,coordinates(CA_spdf),col="blue", cex.lab=0.5)
plot(CA_spdf, add=TRUE)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Add a min population constraint
clus5_min <- skater(edges = ct_mst[,1:2], 
                     data = covariates_scale, 
                     crit = 1000000, 
                     vec.crit = CA_data$TotalPop,
                     ncuts = 4)

CA_data_cluster = (CA_sf %>% mutate(clus = clus5_min$groups))

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")

plot((CA_sf %>% mutate(clus = clus5_min$groups))['clus'], main = "5 cluster example with population constraint")

#True graph structure (adj matrix) produced by SKATER
a1 = c(0,0,1,1,1)
a2 = c(0,0,0,1,0)
a3 = c(1,0,0,0,1)
a4 = c(1,1,0,0,0)
a5 = c(1,0,1,0,0)

adj.true = cbind(a1,a2,a3,a4,a5)
colnames(adj.true) = NULL
```


Determining the optimal number of clusters: 

```{r}
#Nbclust
library(NbClust)

opt.clust = NbClust(covariates_scale,distance = "euclidean",min.nc = 2, 
                    max.nc = 10, method = "kmeans", index = "ch")
opt.clust$All.index
opt.clust$Best.nc
opt.clust$Best.partition

#Silhouette plots 
#(-1 is prob wrong cluster, 0 is right on cusp could go either way, close to 1 is good)

library(cluster)
library (factoextra)

fviz_nbclust(covariates_scale, FUN = hcut, method = 'silhouette') 

#Disassociation matrix (what should we put in there?)
dis = daisy(covariates_scale,metric = "euclidean")

sil = silhouette(clus5_min$groups,dis)
windows()
plot(sil)

```



Graph learning (learn associations between clusters' feature vectors)


HUGE: Learn adjacency matrix A

```{r}
library(huge)

#Practice ex
L=huge.generator(n=2,d=12,graph="cluster",g=4,vis=TRUE)

#Graph path estimation using glasso (produces 10)
est=huge(L$data,method="glasso")

est$path[[5]]
```


```{r}
#Aggregate feature vectors into one vector for each SKATER cluster
CA_cluster = data.frame(CA_sf$NAME,clus5_min$groups)
names(CA_cluster) = c("County","Cluster")

CA_cluster = left_join(CA_cluster,CA_data,by = "County")

cluster1 = CA_cluster %>% filter(Cluster == 1)
cluster2 = CA_cluster %>% filter(Cluster == 2)
cluster3 = CA_cluster %>% filter(Cluster == 3)
cluster4 = CA_cluster %>% filter(Cluster == 4)
cluster5 = CA_cluster %>% filter(Cluster == 5)

#Create new data matrix of aggregated feature vectors 

cluster_features = matrix(NA,nrow = 2,ncol = 5)

for (i in 1:5){
  cluster = CA_cluster %>% filter(Cluster == i)
  cluster_features[1,i] = mean(cluster$Poverty)
  cluster_features[2,i] = mean(cluster$AvgAge)
}

#Graph learning w HUGE 
out = huge(cluster_features,method="glasso")
out$path[[5]]

#Nonparanormal (non parametric normal)
cluster_features.npn = huge.npn(cluster_features)
out.npn = huge(cluster_features.npn,method="glasso")
out.npn$path[[5]]

#Plot huge.plot()
huge.plot(out$path[[5]])
plot(out)

#Plot ROC curves
adj.true = as(adj.true,"CsparseMatrix")

###Why does ROC curve come out linear (basically guessing)
huge.roc(out$path,adj.true)
```



INLA practice: 

```{r}
library(INLA)

formula = log(Poverty) ~ AvgAge
CA_data$ID = 1:nrow(CA_data)


###Basic IID model
INLA.iid <- inla(update(formula, . ~. + f(ID, model = "iid")),
  data = CA_data,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.iid)

###Besag's improper
INLA.besag <- inla(update(formula, . ~. + f(ID, model = "besag", graph = adj)), 
  data = CA_data,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.besag)

###Besag's proper
INLA.besagprop <- inla(update(formula, . ~. 
                              + f(ID, model = "besagproper", graph = adj)), 
  data = CA_data,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.besagprop)

###BYM
INLA.bym <- inla(update(formula, . ~. 
                              + f(ID, model = "bym", graph = adj)), 
  data = CA_data,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.bym)



###Extracting posterior means 
tmarg <- function(marginals) {
  post.means <- mclapply(marginals, function (marg) {
  # Transform post. marginals
  aux <- inla.tmarginal(exp, marg)
  # Compute posterior mean
  inla.emarginal(function(x) x, aux)
  })

  return(as.vector(unlist(post.means)))
}

# Add posterior means to the SpatialPolygonsDataFrame
CA_data$IID <- tmarg(INLA.iid$marginals.fitted.values)
CA_data$besag <- tmarg(INLA.besag$marginals.fitted.values)
CA_data$besagprop <- tmarg(INLA.besagprop$marginals.fitted.values)
CA_data$bym <- tmarg(INLA.bym$marginals.fitted.values)
```


