---
title: "SoA Data Graph Learning"
author: "Jeffrey Wu"
date: "2023-09-11"
output: pdf_document
---

Downloading and formatting data: 

```{r}
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)
library(readxl)
library(spdep)
library(sp)
library(INLA)
library(HMMpa)
library(invgamma)
library(robsel)

soa.data = read_xlsx("SoA.data.1019.xlsx")

###county_flips are unique identifier for counties
soa.data$county_fips = as.character(soa.data$county_fips) ##change it to character

#IMPORTANT

# This shape file contains the coordinates for county boundaries
##counties is from urbanmap

CA.counties = urbnmapr::counties %>% filter(state_abbv == "CA")

CA.counties2 = read.csv("counties.ca.data.csv")
ca.coordinates = data.frame(CA.counties2$county,CA.counties2$lat,CA.counties2$lng)
colnames(ca.coordinates) = c("county","lat","long")

ca.coordinates = ca.coordinates[order(ca.coordinates$county),]
row.names(ca.coordinates) = NULL


###IF WE WANT TO BOIL DOWN TIME SERIES AND KEEP ALL DATA, SWITCH to CA_newdata below
soa_joint <- left_join(CA.counties, soa.data, by = "county_fips")
lat0=min(soa_joint$lat)
lat1=max(soa_joint$lat)

soa_joint %>%
  ggplot(aes(long, lat, group = group, fill = Score)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(65, 185))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Deprivation Score")) +
  ggtitle(paste0("Deprivation Score in California by County"))+
  xlab("lon") +ylab("lat" )


#Use with soa.data (full data)
CA_data = soa_joint %>% select(long, lat, county_name.y, Year, Score, Total_Pop,
                               EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value,
                               Median_Gross_Rent, Housing_No_Telephone,
                               Housing_Incomplete_Plumbing)

colnames(CA_data)[3] = "County"

CA_newdata = soa.data[1:58,]
CA_newdata = CA_newdata[,-c(4,7,8)]
```



SKATER clustering: 

```{r}
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

score_scaled = scale(CA_data$Score)
#unemploy_scaled = scale(CA_data$Unemployment_Rate)

#covariates_scale = data.frame(apply(CA_data[,4:16],2,scale))
covariates_scale = data.frame(score_scaled)

CA_spdf@data = covariates_scale
```


```{r}
#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#summary(CA_nb)

# plot(CA_spdf, main = "With queen")
# plot(CA_nb, coords = coordinates(CA_spdf), col="blue", add = TRUE)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

plot(ct_mst,coordinates(CA_spdf),col="blue", cex.lab=0.5)
plot(CA_spdf, add=TRUE)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Add a min population constraint
clus5_min <- skater(edges = ct_mst[,1:2], 
                     data = covariates_scale, 
                     crit = 5000000, 
                     vec.crit = CA_data$Total_Pop,
                     ncuts = 4)

#Add a minimum number of areas in each cluster constraint 
clus5_minarea = skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4, 8)


CA_data_cluster = (CA_sf %>% mutate(clus = clus5_min$groups))

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")

plot((CA_sf %>% mutate(clus = clus5_min$groups))['clus'], main = "5 cluster example with population constraint")

plot((CA_sf %>% mutate(clus = clus5_minarea$groups))['clus'], main = "5 cluster example with minimum number of areas constraint")



#plot(CA_sf,col=c("red","green","blue","purple","yellow")[clus5_min$groups],max.plot=17)
```



HUGE to learn adjacency matrix A: 


```{r}
library(huge)

#Aggregate feature vectors into one vector for each SKATER cluster
CA_cluster = data.frame(CA_sf$NAMELSAD,clus5$groups)
names(CA_cluster) = c("County","Cluster")
year = 2010:2019

CA_cluster = left_join(CA_cluster,CA_data,by = "County")

cluster1 = CA_cluster %>% filter(Cluster == 1)
cluster2 = CA_cluster %>% filter(Cluster == 2)
cluster3 = CA_cluster %>% filter(Cluster == 3)
cluster4 = CA_cluster %>% filter(Cluster == 4)
cluster5 = CA_cluster %>% filter(Cluster == 5)

# #Create a 10 year time series for each cluster, get avg values for each year 
# CA.ts = list()
# 
# for (i in 1:5){
#   cluster = CA_cluster %>% filter(Cluster == i)
# 
#   CA.avg = matrix(NA,nrow = 10, ncol = 2)
# 
#   for(j in 1:10){
#   vec = cluster %>% filter(Year == year[j]) %>% select(Score)
#   CA.avg[j,] = apply(vec,2,mean)
#   }
# 
#   CA.avg = cbind(year,CA.avg)
#   colnames(CA.avg) = c("Year","Score","Unemployment")
# 
#   CA.ts[[i]] = CA.avg
# }

#Get weighted avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
cluster_features = matrix(NA,nrow = 10,ncol = 5)

for (i in 1:5){
  cluster = CA_cluster %>% filter(Cluster == i)

  for(j in 1:10){
    #Obtain a weighted mean based on population
    vec = cluster %>% filter(Year == year[j]) %>% select(Score,Total_Pop) %>% unique()
    cluster.pop = sum(vec$Total_Pop)
    cluster.popweights = vec$Total_Pop/cluster.pop
    cluster_features[j,i] = weighted.mean(vec$Score,cluster.popweights)
  }
}

#Get avg value for each feature for each cluster 
#Create new data matrix of aggregated feature vectors 
# cluster_features = matrix(NA,nrow = 2,ncol = 5)
# 
# for (i in 1:5){
#   cluster = CA_cluster %>% filter(Cluster == i)
#   vec = cluster %>% select(Score,Unemployment_Rate)
#   cluster_features[,i] = apply(vec,2,mean)
# }

#Graph learning w HUGE

#Robust selection for lambda 
# lambda = robsel(cluster_features,alpha=0.95,B=200)

out.mb = huge(cluster_features,method="mb")
out.glasso = huge(cluster_features,method="glasso")

#adj matrix 
out.mb$path[[5]]
#prec matrix 
out.mb$icov[[5]]

#Select which graph using StARS or RIC? Not sure how this works yet 
mb.stars = huge.select(out.mb,criterion = "stars",stars.thresh = 0.1)
mb.ric = huge.select(out.mb)

glasso.stars = huge.select(out.glasso,criterion = "stars",stars.thresh = 0.1)
glasso.ric = huge.select(out.glasso)
glasso.ebic = huge.select(out.glasso,criterion = "ebic")


#The optimal graph selected from graph path 
mb.stars$refit
mb.ric$refit

#Make some plots 
# plot(mb.stars)
# plot(mb.ric)

plot(glasso.stars)
plot(glasso.ric)
plot(glasso.ebic)

huge.est = glasso.ebic$refit
huge.est

# #Plot huge.plot()
# huge.plot(out$path[[5]])
# plot(out)

# #Plot ROC curves
# adj5.true = as(adj5.true,"CsparseMatrix")
# 
# huge.roc(out$path,adj5.true)


# #Try HUGE on raw data
# raw_features = rbind(CA_newdata$Score,CA_newdata$Unemployment_Rate)
# out2 = huge(raw_features,method = "glasso")
# out2$path[[5]]
```

TRANSFORM ADJ MATRIX A TO GRAPH FILTER H

```{r}
#Function implementing transformation of adj matrix A to graph filter H worth it?? 
# filter.transform = function(Amatrix,q){
#   
# }

A = as.matrix(huge.est)
p = nrow(A)

#obtain graph Laplacian L
D = diag(p)
for (i in 1:p){
  d = sum(A[,i])
  D[i,i] = d
}

L = D - A

#eigendecomposition of L
Ldecomp = eigen(L)
U = as.matrix(Ldecomp$vectors)
Lambdas = Ldecomp$values

#test
#U %*% (diag(p)*Lambdas) %*% t(U)

#Function implementing cutoff tranform for eigenvalues 
cutoff.transform = function(lambdas,q){
  transformed = c()
  cutoff = quantile(lambdas,q)
  for (i in lambdas){
    if(i <= cutoff){
      transformed = c(transformed,1)
    }
    else{
      transformed = c(transformed,0)
    }
  }
  
  return(transformed)
}

#quantile(Lambdas,2/3)
transformed.L = cutoff.transform(Lambdas,2/3)
eta.L = diag(p)*transformed.L

#obtain graph filter
H = U %*% eta.L %*% t(U)
```

