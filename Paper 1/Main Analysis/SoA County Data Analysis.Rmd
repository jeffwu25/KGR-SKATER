---
title: "SoA Data Graph Learning"
author: "Jeffrey Wu"
date: "2023-05-08"
output: pdf_document
---

Downloading and formatting data: 

```{r}
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)
library(readxl)
library(spdep)
library(sp)
library(INLA)
library(HMMpa)
library(invgamma)
library(robsel)

soa.data = read_xlsx("SoA.data.1019.xlsx")

###county_flips are unique identifier for counties
soa.data$county_fips = as.character(soa.data$county_fips) ##change it to character

#IMPORTANT

# This shape file contains the coordinates for county boundaries
##counties is from urbanmap

CA.counties = urbnmapr::counties %>% filter(state_abbv == "CA")

CA.counties2 = read.csv("counties.ca.data.csv")
ca.coordinates = data.frame(CA.counties2$county,CA.counties2$lat,CA.counties2$lng)
colnames(ca.coordinates) = c("county","lat","long")

ca.coordinates = ca.coordinates[order(ca.coordinates$county),]
row.names(ca.coordinates) = NULL


###IF WE WANT TO BOIL DOWN TIME SERIES AND KEEP ALL DATA, SWITCH to CA_newdata below
soa_joint <- left_join(CA.counties, soa.data, by = "county_fips")
lat0=min(soa_joint$lat)
lat1=max(soa_joint$lat)

soa_joint %>%
  ggplot(aes(long, lat, group = group, fill = Score)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(65, 185))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Deprivation Score")) +
  ggtitle(paste0("Deprivation Score in California by County"))+
  xlab("lon") +ylab("lat" )


#Use with soa.data (full data)
CA_data = soa_joint %>% select(long, lat, county_name.y, Year, Score, Total_Pop,
                               EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value,
                               Median_Gross_Rent, Housing_No_Telephone,
                               Housing_Incomplete_Plumbing)

colnames(CA_data)[3] = "County"

CA_newdata = soa.data[1:58,]
CA_newdata = CA_newdata[,-c(4,7,8)]
```



SKATER clustering: 

```{r}
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

score_scaled = scale(CA_data$Score)
#unemploy_scaled = scale(CA_data$Unemployment_Rate)

#covariates_scale = data.frame(apply(CA_data[,4:16],2,scale))
covariates_scale = data.frame(score_scaled)

CA_spdf@data = covariates_scale
```


```{r}
#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#summary(CA_nb)

# plot(CA_spdf, main = "With queen")
# plot(CA_nb, coords = coordinates(CA_spdf), col="blue", add = TRUE)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

plot(ct_mst,coordinates(CA_spdf),col="blue", cex.lab=0.5)
plot(CA_spdf, add=TRUE)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Add a min population constraint
clus5_min <- skater(edges = ct_mst[,1:2], 
                     data = covariates_scale, 
                     crit = 5000000, 
                     vec.crit = CA_data$Total_Pop,
                     ncuts = 4)

#Add a minimum number of areas in each cluster constraint 
clus5_minarea = skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4, 8)


CA_data_cluster = (CA_sf %>% mutate(clus = clus5_min$groups))

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")

plot((CA_sf %>% mutate(clus = clus5_min$groups))['clus'], main = "5 cluster example with population constraint")

plot((CA_sf %>% mutate(clus = clus5_minarea$groups))['clus'], main = "5 cluster example with minimum number of areas constraint")



#plot(CA_sf,col=c("red","green","blue","purple","yellow")[clus5_min$groups],max.plot=17)
```




```{r}
#What is true graph structure (code in actual adjacency matrix for 5 clusters)
a1 = c(0,1,0,1,1)
a2 = c(1,0,0,0,1)
a3 = c(0,0,0,1,0)
a4 = c(1,0,1,0,0)
a5 = c(1,1,0,0,0)

adj5.true = cbind(a1,a2,a3,a4,a5)
colnames(adj5.true) = NULL
```



Determine optimal number of clusters (between 5-10): 
DOESN"T WORK FOR RAW TIME SERIES DATA (NON AGGREGATED)

```{r}
#Silhouette plots 
#(-1 is prob wrong cluster, 0 is right on cusp could go either way, close to 1 is good)

library(cluster)
library (factoextra)

#This one says 2 clusters has largest average silhouette width 
fviz_nbclust(covariates_scale, FUN = hcut, method = 'silhouette') 

#Disassociation matrix (what should we put in there?)
dis = daisy(covariates_scale,metric = "euclidean")

#Silhouette plot
sil = silhouette(clus5_min$groups,dis)
windows()
plot(sil)


#SKATER CLUSTERS 5-10
for (i in 4:9){
  clus.i <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = i)
  plot((CA_sf %>% mutate(clus = clus.i$groups))['clus'], main = "Cluster example with population constraint")
  
  sil = silhouette(clus.i$groups,dis)
  plot(sil)
}

#SKATER 2 CLUSTERS 
clus2 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 1)
clus2_min <- skater(edges = ct_mst[,1:2], 
                     data = covariates_scale, 
                     crit = 10000000, 
                     vec.crit = CA_data$Total_Pop,
                     ncuts = 1)

plot((CA_sf %>% mutate(clus = clus2$groups))['clus'], main = "2 cluster example")
plot((CA_sf %>% mutate(clus = clus2_min$groups))['clus'], main = "2 cluster example with population constraint")
  
sil = silhouette(clus2$groups,dis)
plot(sil)

sil = silhouette(clus2_min$groups,dis)
plot(sil)
```

SEE OTHER RMD FILE FOR EDA ON BEST # CLUSTERS


HUGE to learn adjacency matrix A: 

```{r}
library(huge)

#Aggregate feature vectors into one vector for each SKATER cluster
CA_cluster = data.frame(CA_sf$NAMELSAD,clus5$groups)
names(CA_cluster) = c("County","Cluster")
year = 2010:2019

CA_cluster = left_join(CA_cluster,CA_data,by = "County")

cluster1 = CA_cluster %>% filter(Cluster == 1)
cluster2 = CA_cluster %>% filter(Cluster == 2)
cluster3 = CA_cluster %>% filter(Cluster == 3)
cluster4 = CA_cluster %>% filter(Cluster == 4)
cluster5 = CA_cluster %>% filter(Cluster == 5)

# #Create a 10 year time series for each cluster, get avg values for each year 
# CA.ts = list()
# 
# for (i in 1:5){
#   cluster = CA_cluster %>% filter(Cluster == i)
# 
#   CA.avg = matrix(NA,nrow = 10, ncol = 2)
# 
#   for(j in 1:10){
#   vec = cluster %>% filter(Year == year[j]) %>% select(Score)
#   CA.avg[j,] = apply(vec,2,mean)
#   }
# 
#   CA.avg = cbind(year,CA.avg)
#   colnames(CA.avg) = c("Year","Score","Unemployment")
# 
#   CA.ts[[i]] = CA.avg
# }

#Get weighted avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
cluster_features = matrix(NA,nrow = 10,ncol = 5)

for (i in 1:5){
  cluster = CA_cluster %>% filter(Cluster == i)

  for(j in 1:10){
    #Obtain a weighted mean based on population
    vec = cluster %>% filter(Year == year[j]) %>% select(Score,Total_Pop)
    cluster.pop = sum(vec$Total_Pop)
    cluster.popweights = vec$Total_Pop/cluster.pop
    cluster_features[j,i] = weighted.mean(vec$Score,cluster.popweights)
  }
}

#Get avg value for each feature for each cluster 
#Create new data matrix of aggregated feature vectors 
# cluster_features = matrix(NA,nrow = 2,ncol = 5)
# 
# for (i in 1:5){
#   cluster = CA_cluster %>% filter(Cluster == i)
#   vec = cluster %>% select(Score,Unemployment_Rate)
#   cluster_features[,i] = apply(vec,2,mean)
# }

#Graph learning w HUGE

#Robust selection for lambda 
lambda = robsel(cluster_features,alpha=0.95,B=200)

out.mb = huge(cluster_features,method="mb")
out.glasso = huge(cluster_features,method="glasso")

#adj matrix 
out.mb$path[[5]]
#prec matrix 
out.mb$icov[[5]]

#Select which graph using StARS or RIC? Not sure how this works yet 
mb.stars = huge.select(out.mb,criterion = "stars",stars.thresh = 0.1)
mb.ric = huge.select(out.mb)

glasso.stars = huge.select(out.glasso,criterion = "stars",stars.thresh = 0.1)
glasso.ric = huge.select(out.glasso)
glasso.ebic = huge.select(out.glasso,criterion = "ebic")


#The optimal graph selected from graph path 
mb.stars$refit
mb.ric$refit

#Make some plots 
# plot(mb.stars)
# plot(mb.ric)

plot(glasso.stars)
plot(glasso.ric)
plot(glasso.ebic)

huge.est = glasso.ric$refit
huge.est

# #Plot huge.plot()
# huge.plot(out$path[[5]])
# plot(out)

# #Plot ROC curves
# adj5.true = as(adj5.true,"CsparseMatrix")
# 
# huge.roc(out$path,adj5.true)


# #Try HUGE on raw data
# raw_features = rbind(CA_newdata$Score,CA_newdata$Unemployment_Rate)
# out2 = huge(raw_features,method = "glasso")
# out2$path[[5]]
```

TRANSFORM ADJ MATRIX A TO GRAPH FILTER H

```{r}
#Function implementing transformation of adj matrix A to graph filter H worth it?? 
# filter.transform = function(Amatrix,q){
#   
# }

A = as.matrix(huge.est)
p = nrow(A)

#obtain graph Laplacian L
D = diag(p)
for (i in 1:p){
  d = sum(A[,i])
  D[i,i] = d
}

L = D - A

#eigendecomposition of L
Ldecomp = eigen(L)
U = as.matrix(Ldecomp$vectors)
Lambdas = Ldecomp$values

#test
#U %*% (diag(p)*Lambdas) %*% t(U)

#Function implementing cutoff tranform for eigenvalues 
cutoff.transform = function(lambdas,q){
  transformed = c()
  cutoff = quantile(lambdas,q)
  for (i in lambdas){
    if(i <= cutoff){
      transformed = c(transformed,1)
    }
    else{
      transformed = c(transformed,0)
    }
  }
  
  return(transformed)
}

#quantile(Lambdas,2/3)
transformed.L = cutoff.transform(Lambdas,2/3)
eta.L = diag(p)*transformed.L

#obtain graph filter
H = U %*% eta.L %*% t(U)
```


JUST FOR PRACTICE

GENERATE SOME SYNTHETIC POISSON DIST DATA W LATENT GP

```{r}
library(MASS)

#Doesn't work with full/raw data so use CA_newdata
x1 = CA_newdata$EDUC_college
x2 = CA_newdata$Unemployment_Rate
N = length(x1)

sigma = 1
alpha = 0.5

Ksigma = matrix(data = NA,nrow = N, ncol = N)

for (i in 1:N){
  for (j in 1:N){
    Ksigma[i,j] = exp(-((x1[i]-x1[j])^2 + (x2[i]-x2[j])^2) / (2*sigma^2))
    }
}

#Create time kernel Ktime
time = 2010:2019
Ktime = matrix(NA,10,10)
rho = 0.5

for (i in 1:10){
  for (j in 1:10){
    Ktime[i,j] = exp(-(abs(time[i]-time[j])^2) / (2*rho))
  }
}

#Covariance eq from graph regression paper? 
#Ktime is dim 10x10, huge.est is 5x5 -> 50x50 is not right dim should be 58x58? 
Sigma = kronecker(Ksigma,huge.est^2)

#Set mean 0 generally
lambda0 = 1
mu0 = 1

lambdas = lambda0*mu0*exp(mvrnorm(n=1,rep(0,N),Ksigma))

Hospitalizations = rep(0,N)

for (i in 1:N){
  Hospitalizations[i] = rgenpois(1,lambda1 = lambdas[i],lambda2 = alpha)
}


#Hospitalizations = data.frame(Hospitalizations)
CA_newdata = cbind(CA_newdata,Hospitalizations)
```




FIT INLA MODELS TO THIS TOY DATA: 

```{r}
#What should the formula be?? 
formula = Hospitalizations ~ exp(EDUC_college + Unemployment_Rate)
CA_newdata$ID = 1:nrow(CA_newdata)


###Split into training and test dataset to "impute" missing values 
CA_newdata2 = CA_newdata
CA_newdata2$Hospitalizations[1:5] = NA 

###Add verbose = T for all the details
###Basic IID model
INLA.iid <- inla(update(formula, . ~. + f(ID, model = "iid")),
  family = "gpoisson",data = CA_newdata,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.iid)

###Besag's improper
INLA.besag <- inla(update(formula, . ~. + f(ID, model = "besag", graph = adj)), 
  family = "gpoisson",data = CA_newdata2,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.besag)

###Besag's proper
INLA.besagprop <- inla(update(formula, . ~. 
                              + f(ID, model = "besagproper", graph = adj)), 
  family = "gpoisson",data = CA_newdata,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.besagprop)

###BYM
INLA.bym <- inla(update(formula, . ~. 
                              + f(ID, model = "bym", graph = adj)), 
  family = "gpoisson",data = CA_newdata,
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE,
                         return.marginals.predictor=TRUE),
  control.predictor = list(compute = TRUE)
)
summary(INLA.bym)



###Extracting posterior means 
# tmarg <- function(marginals) {
#   post.means <- mclapply(marginals, function (marg) {
#   # Transform post. marginals
#   aux <- inla.tmarginal(exp, marg)
#   # Compute posterior mean
#   inla.emarginal(function(x) x, aux)
#   })
# 
#   return(as.vector(unlist(post.means)))
# }
# 
# # Add posterior means to the SpatialPolygonsDataFrame
# CA_newdata$IID <- tmarg(INLA.iid$marginals.fitted.values)
# CA_newdata$besag <- tmarg(INLA.besag$marginals.fitted.values)
# CA_newdata$besagprop <- tmarg(INLA.besagprop$marginals.fitted.values)
# CA_newdata$bym <- tmarg(INLA.bym$marginals.fitted.values)

#Summary of hyperparameters
INLA.besag$summary.hyperpar
INLA.besag$joint.hyper
INLA.besag$marginals.hyperpar

#Posterior predictive distribution
View(INLA.besag$summary.fitted.values)
View(INLA.besagprop$summary.fitted.values)
View(INLA.bym$summary.fitted.values)
```


