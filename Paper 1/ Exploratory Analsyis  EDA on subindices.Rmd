---
title: "EDA on SOA subindices"
author: "Jeffrey Wu"
date: "2023-05-17"
output: pdf_document
---

Downloading and formatting data: 

```{r}
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)
library(readxl)
library(spdep)
library(sp)
library(INLA)
library(HMMpa)
library(invgamma)

soa.data = read_xlsx("SoA.data.1019.xlsx")

###county_flips are unique identifier for counties
soa.data$county_fips = as.character(soa.data$county_fips) ##change it to character

###Boil down each feature's time series (2010-2019) to summary stat eg mean 
###Could ALSO estimate a time series model for each county and cluster on parameter values for each county (another form of dimension reduction)
CA_newdata = soa.data[1:58,]
CA_newdata = CA_newdata[,-c(4,7,8)]
countynames = CA_newdata$county_name

###Practice on LA county
# LA = soa.data %>% filter(county_name == "Los Angeles County") %>%
#   select(Score,Total_Pop, EDUC_Lessthan9, EDUC_college, White_Collar,
#                                Unemployment_Rate, Adj_HH_income, Income_Disparity,
#                                Individuals_Below_Poverty, Median_Home_Value, 
#                                Median_Gross_Rent, Housing_No_Telephone, 
#                                Housing_Incomplete_Plumbing)
# test = apply(LA,2,mean)
# 
# for (i in 5:17){
#   CA_newdata[19,i] = test[i-4]
# }

for (i in 1:58){
  features = soa.data %>% filter(county_name == countynames[i]) %>% 
    select(Score,Total_Pop, EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value, 
                               Median_Gross_Rent, Housing_No_Telephone, 
                               Housing_Incomplete_Plumbing) 
  features.avg = apply(features,2,mean)
  for (j in 5:17){
    CA_newdata[i,j] = features.avg[j-4]
  }
}

#IMPORTANT

# This shape file contains the coordinates for county boundaries
##counties is from urbanmap

CA.counties = counties %>% filter(state_abbv == "CA")

CA.counties2 = read.csv("counties.ca.data.csv")
ca.coordinates = data.frame(CA.counties2$county,CA.counties2$lat,CA.counties2$lng)
colnames(ca.coordinates) = c("county","lat","long")

ca.coordinates = ca.coordinates[order(ca.coordinates$county),]
row.names(ca.coordinates) = NULL


###IF WE DON"T WANT TO BOIL DOWN TIME SERIES AND KEEP ALL DATA, SWITCH CA_newdata w soa.data below 


soa_joint <- left_join(CA.counties, soa.data, by = "county_fips")
lat0=min(soa_joint$lat)
lat1=max(soa_joint$lat)

soa_joint %>%
  ggplot(aes(long, lat, group = group, fill = Score)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(65, 185))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Deprivation Score")) +
  ggtitle(paste0("Deprivation Score in California by County"))+
  xlab("lon") +ylab("lat" )


CA_newdata = cbind(CA_newdata,ca.coordinates[,2:3])


CA_data = CA_newdata %>% select(long, lat, county_name, Score, Total_Pop, 
                               EDUC_Lessthan9, EDUC_college, White_Collar,
                               Unemployment_Rate, Adj_HH_income, Income_Disparity,
                               Individuals_Below_Poverty, Median_Home_Value, 
                               Median_Gross_Rent, Housing_No_Telephone, 
                               Housing_Incomplete_Plumbing) 

colnames(CA_data)[3] = "County"
```


EDA on Score: 

```{r}
CA.totalpop = sum(soa.data$Total_Pop)

#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Score)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Score)
sd(soa2010$Score)
hist(soa2010$Score) #slight skew right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Score,weights2010)


#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Score)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Score)
sd(soa2017$Score)
hist(soa2017$Score) #slight skew right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Score,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Score)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "SDI Score")


lat0=min(soa2017$lat)
lat1=max(soa2017$lat)

soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Score)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 185))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Deprivation Score")) +
  ggtitle(paste0("Deprivation Score in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Score) #yes

qqPlot(soa2017$Score) #yes

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
scores.avg = rep(0,10)
scores.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  scores.avg[i] = mean(vec$Score)
  scores.wavg[i] = weighted.mean(vec$Score,weights)
}

scores.avg = ts(scores.avg,start = 2010,end = 2019)
scores.wavg = ts(scores.wavg,start = 2010,end = 2019)

plot(scores.avg) #Too few lag points but seems stationary 
plot(acf(scores.avg)) #No significant autocorrelations it appears
plot(pacf(scores.avg)) #No significant PACFs

###SHOULD DO WEIGHTED AVG TOO I GUESS 

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.scores.ts = ts(LA.data$Score,start=2010,end=2019)

plot(LA.scores.ts)
plot(acf(LA.scores.ts))
plot(pacf(LA.scores.ts))

#Calculate spatial autocorrelation 
moran.test(soa2010$Score,ct_w) #-0.06 no significant spatial autocorrelation
```


Generate more data 2 ways and compare huge estimations: 

```{r}
###STANDARD BOOTSTRAP (nonparametric)
score.data = df[,2:11]
sample.idx = round(runif(100,1,10))

score.boot = score.data[,sample.idx]
colnames(score.boot) = NULL
#score.boot = cbind(soa.data$county_name[1:58],score.boot)

#HUGE graph estimation of full adj matrix 
library(huge)

out.boot = huge(t(score.boot),method="glasso")
#out.boot$path[[5]]
#out.boot$icov[[5]]

#est optimal adj matrix
boot.ric = huge.select(out.boot,criterion = "ric")
plot(boot.ric)

#est optimal precision matrix 
boot.ric$opt.icov
norm(boot.ric$opt.icov) #norm is about 0.5

###SKATER clustering -> HUGE graph estimation of full adj matrix 

###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

#I feel like it should be 58 rows, 100 columns since each column is treated as its own feature 
score_scaled = scale(score.boot)
covariates_scale = data.frame(score_scaled)

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")


#Get avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
CA_cluster = data.frame(CA_sf$NAMELSAD,clus5$groups)
names(CA_cluster) = c("County","Cluster")

CA_cluster = rbind(clus5$groups,t(score.boot))

cluster_features_boot = matrix(NA,nrow = 100,ncol = 5)

for (i in 1:5){
  idx = which(CA_cluster[1,] == i)
  
  for(j in 1:100){
  sum = sum(CA_cluster[j+1,idx])
  avg = sum / length(idx)
  cluster_features_boot[j,i] = avg
  }
}

#Graph learning w HUGE
out.skboot = huge(cluster_features_boot,method="glasso")
# out.skboot$path[[5]]
# out.skboot$icov[[5]]

#est optimal adj matrix
skboot.stars = huge.select(out.skboot,criterion = "stars")
skboot.stars$refit
plot(skboot.stars)

skboot.stars$icov

#est optimal precision matrix 
skboot.stars$opt.icov #why is this null for ric method? 
norm(skboot.stars$opt.icov) #norm is about 0.978 for stars 
```


Only one edge missing (1 to 3)

Clusters breakdown: 1  2  3  4  5 
                    32  4  7 14  1 



```{r}
###GP KRIGING (parametric)
library(GauPro)
x = c(2010:2019)
xtest =seq(2010,2019,length=100)
preds.gp = matrix(NA,nrow = 100,ncol = 58)

# y = score.data[1,]
# score.gpmod = GauPro(x,as.numeric(y), kernel = kern.exp, parallel = FALSE)
# plot(x,y)
# curve(score.gpmod$predict(x),add = T, col = 2)
# preds = score.gpmod$predict(xtest)

kern.exp = Exponential$new(0)
kern.matern = Matern52$new(0)

for(i in 1:58){
  y = score.data[i,]
  score.gpmod = GauPro(x,as.numeric(y), kernel = kern.matern, parallel = FALSE)
  preds.gp[,i] = score.gpmod$predict(xtest)
}

#Got errors/warnings with the fit but looks fine for random counties 
a = round(runif(1,1,58))
plot(x,score.data[a,])
plot(xtest,preds.gp[,a])

#HUGE graph estimation of full adj matrix 
out.gp = huge(preds.gp,method="glasso")
# out.gp$path[[5]]
# out.gp$icov[[5]]


#est optimal adj matrix
gp.ric = huge.select(out.gp,criterion = "ric")
gp.ric$refit
plot(gp.ric)

#est optimal precision matrix 
gp.ric$opt.icov #why is this null for ric method???
norm(gp.ric$opt.icov) #norm is 0.5 for stars


###SKATER clustering -> HUGE graph estimation of full adj matrix 
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

score_scaled = scale(preds.gp)
covariates_scale = data.frame(t(score_scaled))

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")


#HUGE estimation on clustered groups 
#Get avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
CA_cluster = rbind(clus5$groups,preds.gp)

cluster_features_gp = matrix(NA,nrow = 100,ncol = 5)

for (i in 1:5){
  idx = which(CA_cluster[1,] == i)
  
  for(j in 1:100){
  sum = sum(CA_cluster[j+1,idx])
  avg = sum / length(idx)
  cluster_features_gp[j,i] = avg
  }
}

#Graph learning w HUGE
out.skgp = huge(cluster_features_gp,method="glasso",cov.output = T)
# out.skgp$path[[5]]
# out.skgp$icov[[5]]


#est optimal adj matrix
skgp.stars = huge.select(out.skgp,criterion = "stars")
skgp.stars$refit
plot(skgp.stars)

#est optimal precision matrix 
skgp.stars$opt.icov #why is this null for ric method???
norm(skgp.stars$opt.icov) #norm is 0.5016994 for stars
```

HUGE estimates completely connected graph 

SKATER clustering under GP kriging: 1  2  3  4  5 
                                    33  3  9  9  4 



Hypothesis testing for 2 covariance matrices (from Cai 2013 paper)

H0: 2 sample covariances are equal

Function implementing 2 equal covariance matrix hypothesis testing: 

```{r}
twocovtest = function(data1,data2,alpha){
  p = ncol(data1)
  n = nrow(data1)
  data1 = scale(data1)
  data2 = scale(data2)
  
  xbars = apply(data1,2,mean)
  ybars = apply(data2,2,mean)
  scov1 = matrix(NA,p,p)
  scov2 = matrix(NA,p,p)

  #calculating covariances between each cluster group
  for (i in 1:p){
    for (j in i:p){
      #calculating covariances between each cluster group
      scov1[i,j] = t(data1[,i] - xbars[i]) %*% (data1[,j] - xbars[j]) / n
      scov1[j,i] = scov1[i,j]
      
      scov2[i,j] = t(data2[,i] - ybars[i]) %*% (data2[,j] - ybars[j]) / n
      scov2[j,i] = scov2[i,j]
    }
  }
  
  #calculate corresponding thetas
  theta1hat = matrix(NA,p,p)
  theta2hat = matrix(NA,p,p)
  
  for (i in 1:p){
    for (j in i:p){
      #calculating covariances between each cluster group
      theta1hat[i,j] = sum((((data1[,i] - xbars[i])*(data1[,j] - xbars[j])) - scov1[i,j])^2) / n
      
      theta2hat[i,j] = sum((((data2[,i] - xbars[i])*(data2[,j] - xbars[j])) - scov2[i,j])^2) / n
      
      theta1hat[j,i] = theta1hat[i,j]
      theta2hat[j,i] = theta2hat[i,j]
    }
  }
  
  #calculate M statistics
  W = matrix(NA,p,p)
  W = (scov1 - scov2) / sqrt((theta1hat/n) + (theta2hat/n))
  
  M = W^2
  TS = max(M)
  
  #use extreme value distribution  
  qalpha = -log(8*pi) - 2*log(log((1-alpha)^-1))
  CV = qalpha + (4*log(p)) - (log(log(p)))
  
  #reject hypothesis or not? REJECT IF TRUE 
  return(c(TS,CV))
  #return(CV)
  #return((TS > CV) || (TS == CV))
}

twocovtest(t(score.boot),preds.gp,0.05)
twocovtest(cluster_features_boot,cluster_features_gp,0.05)
```

For non clustered data: 

Fail to reject H0 so we conclude that the 2 resampling methods resulted in HUGE estimated graphs that have similar (not significantly significantly different) sample covariance matrices

For clustered data: 

Reject H0 so we conclude that the 2 resampling methods resulted in HUGE estimated graphs (on the clustered features) that have statistically significantly different sample covariance matrices




(Hardcoded version)

```{r}
###Steps 

#obtain sample covariance matrices 
p = nrow(Sigmahat.skboot)
n = 100
head(Sigmahat.skboot)
head(Sigmahat.skgp)

cluster_features_boot = scale(cluster_features_boot)
cluster_features_gp = scale(cluster_features_gp)

xbars = apply(cluster_features_boot,2,mean)
ybars = apply(cluster_features_gp,2,mean)
scov.boot = matrix(NA,5,5)
scov.gp = matrix(NA,5,5)

#calculating covariances between each cluster group
for (i in 1:p){
  for (j in i:p){
    scov.boot[i,j] = t(cluster_features_boot[,i] - xbars[i]) %*% (cluster_features_boot[,j] - xbars[j]) / n
    scov.boot[j,i] = scov.boot[i,j]
    
    scov.gp[i,j] = t(cluster_features_gp[,i] - ybars[i]) %*% (cluster_features_gp[,j] - ybars[j]) / n
    scov.gp[j,i] = scov.gp[i,j]
  }
}

#calculate corresponding thetas
theta1hat = matrix(NA,5,5)
theta2hat = matrix(NA,5,5)

for (i in 1:p){
  for (j in i:p){
    #calculating covariances between each cluster group
    theta1hat[i,j] = sum((((cluster_features_boot[,i] - xbars[i])*(cluster_features_boot[,j] - xbars[j])) - scov.boot[i,j])^2) / n
    
    theta2hat[i,j] = sum((((cluster_features_gp[,i] - xbars[i])*(cluster_features_gp[,j] - xbars[j])) - scov.gp[i,j])^2) / n
    
    theta1hat[j,i] = theta1hat[i,j]
    theta2hat[j,i] = theta2hat[i,j]
  }
}

#calculate M statistics
W = matrix(NA,5,5)

W = (scov.boot - scov.gp) / sqrt((theta1hat/n) + (theta2hat/n))

M = W^2
TS = max(M)

#use extreme value distribution  
alpha = 0.05
qalpha = -log(8*pi) - 2*log(log((1-alpha)^-1))
CV = qalpha + (4*log(p)) - (log(log(p)))

#reject hypothesis or not? 
(TS > CV) || (TS == CV)
```

EDA on EDUC_Lessthan9 (Percentage of the population 25 years and above with less than nine years of education): 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,EDUC_Lessthan9)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$EDUC_Lessthan9)
sd(soa2010$EDUC_Lessthan9)
hist(soa2010$EDUC_Lessthan9) #slight skew right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$EDUC_Lessthan9,weights2010)


#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,EDUC_Lessthan9)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$EDUC_Lessthan9)
sd(soa2017$EDUC_Lessthan9)
hist(soa2017$EDUC_Lessthan9) #skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$EDUC_Lessthan9,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(EDUC_Lessthan9)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "EDUC_Lessthan9")


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = EDUC_Lessthan9)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 0.25))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% Less than 9 Years of School")) +
  ggtitle(paste0("Education Attainment in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$EDUC_Lessthan9) #Not quite normal
qqPlot(soa2017$EDUC_Lessthan9) #Not quite normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
edu9.avg = rep(0,10)
edu9.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  edu9.avg[i] = mean(vec$EDUC_Lessthan9)
  edu9.wavg[i] = weighted.mean(vec$EDUC_Lessthan9,weights)
}

edu9.avg = ts(edu9.avg,start = 2010,end = 2019)
edu9.wavg = ts(edu9.wavg,start = 2010,end = 2019)

plot(edu9.avg) #Too few lag points, not stationary though (decreasing)
plot(diff(edu9.avg,1))
plot(acf(edu9.avg)) #ACF at lag 1 is significant 
plot(pacf(edu9.avg))

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.edu9.ts = ts(LA.data$EDUC_Lessthan9,start=2010,end=2019)

plot(LA.edu9.ts)
plot(acf(LA.edu9.ts))
plot(pacf(LA.edu9.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$EDUC_Lessthan9,ct_w) #-0.06 no significant spatial autocorrelation
```


EDA on EDUC_college (Percentage of the population 25 years and above with at least a high school diploma): 

```{r}
#Summary stats for each variable for each year 

soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,EDUC_college)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$EDUC_college)
sd(soa2010$EDUC_college)
hist(soa2010$EDUC_college) #skewed right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$EDUC_college,weights2010)


#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,EDUC_college)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$EDUC_college)
sd(soa2017$EDUC_college)
hist(soa2017$EDUC_college) #skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$EDUC_college,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #False

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(EDUC_college)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "EDUC_college")


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = EDUC_college)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 0.6))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% With High School Degree or More")) +
  ggtitle(paste0("Education Attainment in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$EDUC_college) #Not quite normal
qqPlot(soa2017$EDUC_college) #Not quite normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
edu.avg = rep(0,10)
edu.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  edu.avg[i] = mean(vec$EDUC_college)
  edu.wavg[i] = weighted.mean(vec$EDUC_college,weights)
}

edu.avg = ts(edu.avg,start = 2010,end = 2019)
edu.wavg = ts(edu.wavg,start = 2010,end = 2019)

plot(edu.avg) #Too few lag points, not stationary though (increasing)
plot(diff(edu.avg,1))
plot(acf(edu.avg)) #ACF at lag 1 is significant 
plot(pacf(edu.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.edu.ts = ts(LA.data$EDUC_college,start=2010,end=2019)

plot(LA.edu.ts) #increasing trend
plot(acf(LA.edu.ts))
plot(pacf(LA.edu.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$EDUC_college,ct_w) #-0.06 no significant spatial autocorrelation
```



EDA on White_Collar (Percentage of the population 16 years and above employed in white collar occupations): 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,White_Collar)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$White_Collar)
sd(soa2010$White_Collar)
hist(soa2010$White_Collar) #normal-ish distributed 

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$White_Collar,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,White_Collar)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$White_Collar)
sd(soa2017$White_Collar)
hist(soa2017$White_Collar) #normal ish skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$White_Collar,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(White_Collar)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "White_Collar")


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = White_Collar)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0,0.8))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% White Collar Workers")) +
  ggtitle(paste0("% White Collar Workers in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$White_Collar) #Normal
qqPlot(soa2017$White_Collar) #Normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
whitecollar.avg = rep(0,10)
whitecollar.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  whitecollar.avg[i] = mean(vec$White_Collar)
  whitecollar.wavg[i] = weighted.mean(vec$White_Collar,weights)
}

whitecollar.avg = ts(whitecollar.avg,start = 2010,end = 2019)
whitecollar.wavg = ts(whitecollar.wavg,start = 2010,end = 2019)

whitecollar.avg
whitecollar.wavg


plot(whitecollar.avg) #Too few lag points, seems generally stationary
plot(diff(whitecollar.avg,1))
plot(acf(whitecollar.avg)) #ACF at lag 1 is significant 
plot(pacf(whitecollar.avg)) #No significant PACFs 

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.whitecollar.ts = ts(LA.data$White_Collar,start=2010,end=2019)

plot(LA.whitecollar.ts) #seems stationary
plot(acf(LA.whitecollar.ts)) #ACF significant at lag 1 
plot(pacf(LA.whitecollar.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$White_Collar,ct_w) #-0.06 no significant spatial autocorrelation
```




EDA on Unemployment_Rate: 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Unemployment_Rate)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Unemployment_Rate)
sd(soa2010$Unemployment_Rate)
hist(soa2010$Unemployment_Rate) #bimodal

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Unemployment_Rate,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Unemployment_Rate)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Unemployment_Rate)
sd(soa2017$Unemployment_Rate)
hist(soa2017$Unemployment_Rate) #skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Unemployment_Rate,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Unemployment_Rate)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Unemployment_Rate") #def some movement 


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Unemployment_Rate)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 0.2))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Unemployment Rate")) +
  ggtitle(paste0("Unemployment Rates in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Unemployment_Rate) #Basically normal
qqPlot(soa2017$Unemployment_Rate) #Basically normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
unemployment.avg = rep(0,10)
unemployment.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  unemployment.avg[i] = mean(vec$Unemployment_Rate)
  unemployment.wavg[i] = weighted.mean(vec$Unemployment_Rate,weights)
}

unemployment.avg = ts(unemployment.avg,start = 2010,end = 2019)
unemployment.wavg = ts(unemployment.wavg,start = 2010,end = 2019)

unemployment.avg
unemployment.wavg


plot(unemployment.avg) #Too few lag points, seems generally stationary
plot(diff(unemployment.avg,1))
plot(acf(unemployment.avg)) #ACF at lag 1 is significant 
plot(pacf(unemployment.avg)) #No significant PACFs 

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.unemployment.ts = ts(LA.data$Unemployment_Rate,start=2010,end=2019)

plot(LA.unemployment.ts) #seems stationary
plot(acf(LA.unemployment.ts)) #ACF significant at lag 1 
plot(pacf(LA.unemployment.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Unemployment_Rate,ct_w) #-0.06 no significant spatial autocorrelation
```




EDA on Adj_HH_income (Median family income): 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Adj_HH_income)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Adj_HH_income)
sd(soa2010$Adj_HH_income)
hist(soa2010$Adj_HH_income) #skewed right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Adj_HH_income,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Adj_HH_income)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Adj_HH_income)
sd(soa2017$Adj_HH_income)
hist(soa2017$Adj_HH_income) #skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Adj_HH_income,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #False

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Adj_HH_income)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Adj_HH_income") #more outliers over time  


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Adj_HH_income)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 80000))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% White Collar")) +
  ggtitle(paste0("% White Collar Rates in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Adj_HH_income) #Basically normal
qqPlot(soa2017$Adj_HH_income) #Basically normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
income.avg = rep(0,10)
income.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  income.avg[i] = mean(vec$Adj_HH_income)
  income.wavg[i] = weighted.mean(vec$Adj_HH_income,weights)
}

income.avg = ts(income.avg,start = 2010,end = 2019)
income.wavg = ts(income.wavg,start = 2010,end = 2019)

income.avg
income.wavg


plot(income.avg) #Too few lag points, not stationary (increasing trend)
plot(diff(income.avg,1))
plot(acf(income.avg)) #ACF at lag 1 is significant 
plot(pacf(income.avg)) #No significant PACFs 

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.income.ts = ts(LA.data$Adj_HH_income,start=2010,end=2019)

plot(LA.income.ts) #seems stationary
plot(acf(LA.income.ts)) #ACF significant at lag 1 
plot(pacf(LA.income.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Adj_HH_income,ct_w) #-0.06 no significant spatial autocorrelation
```


EDA on Income_Disparity (Ratio of total households with less than 10,000 family income to those with greater than or equal to 50,000 family income a year): 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Income_Disparity)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Income_Disparity)
sd(soa2010$Income_Disparity)
hist(soa2010$Income_Disparity) #skewed right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Income_Disparity,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Income_Disparity)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Income_Disparity)
sd(soa2017$Income_Disparity)
hist(soa2017$Income_Disparity) #skewed right

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Income_Disparity,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #False

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Income_Disparity)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Income_Disparity") #many outliers, slight upward trend 


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Income_Disparity)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 30))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Income Disparity Ratio")) +
  ggtitle(paste0("Income Disparity in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Income_Disparity) #Basically normal
qqPlot(soa2017$Income_Disparity) #Not normal (heavy tailed)

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
disparity.avg = rep(0,10)
disparity.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  disparity.avg[i] = mean(vec$Income_Disparity)
  disparity.wavg[i] = weighted.mean(vec$Income_Disparity,weights)
}

disparity.avg = ts(disparity.avg,start = 2010,end = 2019)
disparity.wavg = ts(disparity.wavg,start = 2010,end = 2019)

disparity.avg
disparity.wavg


plot(disparity.avg) #Too few lag points, not stationary (increasing trend)
plot(diff(disparity.avg,1))
plot(acf(disparity.avg)) #ACF at lag 1 is significant 
plot(pacf(disparity.avg)) #No significant PACFs 

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.disparity.ts = ts(LA.data$Income_Disparity,start=2010,end=2019)

plot(LA.disparity.ts) #seems stationary
plot(acf(LA.disparity.ts)) #ACF significant at lag 1 
plot(pacf(LA.disparity.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Income_Disparity,ct_w) #-0.06 no significant spatial autocorrelation
```




EDA on Individuals_Below_Poverty (Percentage of families below the federal poverty level): 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Individuals_Below_Poverty)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Individuals_Below_Poverty)
sd(soa2010$Individuals_Below_Poverty)
hist(soa2010$Individuals_Below_Poverty) #relatively symmetric not normal though

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Individuals_Below_Poverty,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Individuals_Below_Poverty)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Individuals_Below_Poverty)
sd(soa2017$Individuals_Below_Poverty)
hist(soa2017$Individuals_Below_Poverty) #relatively normal

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Individuals_Below_Poverty,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #False

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Individuals_Below_Poverty)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Individuals_Below_Poverty") #basically the same    


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Individuals_Below_Poverty)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0, 0.3))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% Below Poverty Line")) +
  ggtitle(paste0("Poverty Rate in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Individuals_Below_Poverty) #Basically normal
qqPlot(soa2017$Individuals_Below_Poverty) #Basically normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
belowpov.avg = rep(0,10)
belowpov.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  belowpov.avg[i] = mean(vec$Individuals_Below_Poverty)
  belowpov.wavg[i] = weighted.mean(vec$Individuals_Below_Poverty,weights)
}

belowpov.avg = ts(belowpov.avg,start = 2010,end = 2019)
belowpov.wavg = ts(belowpov.wavg,start = 2010,end = 2019)

belowpov.avg
belowpov.wavg


plot(belowpov.avg) #Too few lag points, basically stationary
plot(diff(belowpov.avg,1))
plot(acf(belowpov.avg)) #No significant ACFs 
plot(pacf(belowpov.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.poverty.ts = ts(LA.data$Individuals_Below_Poverty,start=2010,end=2019)

plot(LA.poverty.ts) #seems stationary
plot(acf(LA.poverty.ts)) #ACF significant at lag 1 
plot(pacf(LA.poverty.ts))

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Individuals_Below_Poverty,ct_w) #-0.06 no significant spatial autocorrelation
```




EDA on Median_Home_Value: 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Median_Home_Value)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Median_Home_Value)
sd(soa2010$Median_Home_Value)
hist(soa2010$Median_Home_Value) #skewed right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Median_Home_Value,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Median_Home_Value)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Median_Home_Value)
sd(soa2017$Median_Home_Value)
hist(soa2017$Median_Home_Value) #Big skew right 

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Median_Home_Value,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Median_Home_Value)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Median_Home_Value") #right tail variation 


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Median_Home_Value)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0,1000000))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Median Home Price")) +
  ggtitle(paste0("Median Home Prices in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Median_Home_Value) #Not normal (left tail)
qqPlot(soa2017$Median_Home_Value) #Not normal (both tails)

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
house.avg = rep(0,10)
house.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  house.avg[i] = mean(vec$Median_Home_Value)
  house.wavg[i] = weighted.mean(vec$Median_Home_Value,weights)
}

house.avg = ts(house.avg,start = 2010,end = 2019)
house.wavg = ts(house.wavg,start = 2010,end = 2019)

house.avg
house.wavg


plot(house.avg) #Too few lag points, basically stationary
plot(diff(house.avg,1))
plot(acf(house.avg)) #No significant ACFs 
plot(pacf(house.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.house.ts = ts(LA.data$Median_Home_Value,start=2010,end=2019)

plot(LA.house.ts) #seems stationary
plot(acf(LA.house.ts)) #No significant ACFs
plot(pacf(LA.house.ts)) #No significant PACFs

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Median_Home_Value,ct_w) #-0.06 no significant spatial autocorrelation
```




EDA on Median_Gross_Rent: 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Median_Gross_Rent)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Median_Gross_Rent)
sd(soa2010$Median_Gross_Rent)
hist(soa2010$Median_Gross_Rent) #kinda symmetric (right skew), bimodal

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Median_Gross_Rent,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Median_Gross_Rent)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Median_Gross_Rent)
sd(soa2017$Median_Gross_Rent)
hist(soa2017$Median_Gross_Rent) #Big skew right 

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Median_Gross_Rent,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #False

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Median_Gross_Rent)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Median_Gross_Rent") #right tail gets longer  


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Median_Gross_Rent)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0,2000))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("Median Rent")) +
  ggtitle(paste0("Median Gross Rent in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Median_Gross_Rent) #Basically normal
qqPlot(soa2017$Median_Gross_Rent) #Not normal (both tails)

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
rent.avg = rep(0,10)
rent.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  rent.avg[i] = mean(vec$Median_Gross_Rent)
  rent.wavg[i] = weighted.mean(vec$Median_Gross_Rent,weights)
}

rent.avg = ts(rent.avg,start = 2010,end = 2019)
rent.wavg = ts(rent.wavg,start = 2010,end = 2019)

rent.avg
rent.wavg


plot(rent.avg) #Too few lag points, not stationary (increasing trend)
plot(diff(rent.avg,1))
plot(acf(rent.avg)) #No significant ACFs 
plot(pacf(rent.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.rent.ts = ts(LA.data$Median_Gross_Rent,start=2010,end=2019)

plot(LA.rent.ts) #not stationary
plot(acf(LA.rent.ts)) #No significant ACFs
plot(pacf(LA.rent.ts)) #No significant PACFs

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Median_Gross_Rent,ct_w) #-0.06 no significant spatial autocorrelation
```



EDA on Housing_No_Telephone: 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Housing_No_Telephone)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Housing_No_Telephone)
sd(soa2010$Housing_No_Telephone)
hist(soa2010$Housing_No_Telephone) #slight skew right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Housing_No_Telephone,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Housing_No_Telephone)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Housing_No_Telephone)
sd(soa2017$Housing_No_Telephone)
hist(soa2017$Housing_No_Telephone) #Symmetric, maybe outlier 

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Housing_No_Telephone,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Housing_No_Telephone)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Housing_No_Telephone") #different spreads   


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Housing_No_Telephone)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0,0.04))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% Without Telephone")) +
  ggtitle(paste0("% Households without Telephone in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Housing_No_Telephone) #Basically normal
qqPlot(soa2017$Housing_No_Telephone) #Basically normal

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
nophone.avg = rep(0,10)
nophone.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  nophone.avg[i] = mean(vec$Housing_No_Telephone)
  nophone.wavg[i] = weighted.mean(vec$Housing_No_Telephone,weights)
}

nophone.avg = ts(nophone.avg,start = 2010,end = 2019)
nophone.wavg = ts(nophone.wavg,start = 2010,end = 2019)

nophone.avg
nophone.wavg


plot(nophone.avg) #Too few lag points, not stationary (decreasing trend)
plot(diff(nophone.avg,1))
plot(acf(nophone.avg)) #No significant ACFs 
plot(pacf(nophone.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.phone.ts = ts(LA.data$Housing_No_Telephone,start=2010,end=2019)

plot(LA.phone.ts) #not stationary
plot(acf(LA.phone.ts)) #No significant ACFs
plot(pacf(LA.phone.ts)) #No significant PACFs

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Housing_No_Telephone,ct_w) #-0.06 no significant spatial autocorrelation
```






EDA on Housing_Incomplete_Plumbing: 

```{r}
#Summary stats for each variable for each year 
soa2010 = soa.data %>% filter(Year == 2010) %>% select(Total_Pop,Housing_Incomplete_Plumbing)
soa2010 = cbind(ca.coordinates,soa2010)
totalpop2010 = sum(soa2010$Total_Pop)

summary(soa2010$Housing_Incomplete_Plumbing)
sd(soa2010$Housing_Incomplete_Plumbing)
hist(soa2010$Housing_Incomplete_Plumbing) #skew right

weights2010 = soa2010$Total_Pop/totalpop2010
popweighted.avg.2010 = weighted.mean(soa2010$Housing_Incomplete_Plumbing,weights2010)

#Compare with 2017
soa2017 = soa.data %>% filter(Year == 2017) %>% select(Total_Pop,Housing_Incomplete_Plumbing)
soa2017 = cbind(ca.coordinates,soa2017)
totalpop2017 = sum(soa2017$Total_Pop)

summary(soa2017$Housing_Incomplete_Plumbing)
sd(soa2017$Housing_Incomplete_Plumbing)
hist(soa2017$Housing_Incomplete_Plumbing) #Big skew right 

weights2017 = soa2017$Total_Pop/totalpop2017
popweighted.avg.2017 = weighted.mean(soa2017$Housing_Incomplete_Plumbing,weights2017)

popweighted.avg.2010 >= popweighted.avg.2017 #True

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Housing_Incomplete_Plumbing)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015","2016","2017","2018","2019")

boxplot(df[,2:11], xlab = "Year",ylab = "Housing_Incomplete_Plumbing") #many outliers   


soa_joint %>% filter(Year == 2017) %>%
  ggplot(aes(long, lat, group = group, fill = Housing_Incomplete_Plumbing)) +
  scale_fill_gradient(low = "yellow", high = "red", na.value = "grey90",limits=c(0,0.025))+  ###here limits set an upper and lower bound
  geom_polygon(col = "black") +
  coord_map(projection = "albers", lat0 = lat0, lat1 = lat1) +  ###lat0 and lat1 are how wide you should draw
  labs(fill = expression("% Without Complete Plumbing")) +
  ggtitle(paste0("% Households without Complete Plumbing in California by County for 2017"))+
  xlab("lon") +ylab("lat" )

#Normal distributed? 
library(car)
qqPlot(soa2010$Housing_Incomplete_Plumbing) #Not normal (right tail)
qqPlot(soa2017$Housing_Incomplete_Plumbing) #Not normal (right tail)

#Generate a time series for avg of each variable -> stationarity, ACF, PACF 
year = c(2010:2019)
noplumbing.avg = rep(0,10)
noplumbing.wavg = rep(0,10)

for (i in 1:10){
  vec = soa.data %>% filter(Year == year[i])
  pop = sum(vec$Total_Pop)
  weights = vec$Total_Pop/pop
  noplumbing.avg[i] = mean(vec$Housing_Incomplete_Plumbing)
  noplumbing.wavg[i] = weighted.mean(vec$Housing_Incomplete_Plumbing,weights)
}

noplumbing.avg = ts(noplumbing.avg,start = 2010,end = 2019)
noplumbing.wavg = ts(noplumbing.wavg,start = 2010,end = 2019)

noplumbing.avg
noplumbing.wavg


plot(noplumbing.avg) #Too few lag points, not stationary (decreasing trend)
plot(diff(noplumbing.avg,1))
plot(acf(noplumbing.avg)) #Significant ACF at lag 1  
plot(pacf(noplumbing.avg)) #No significant PACFs

#Time series for LA county 

LA.data = soa.data %>% filter(county_name == "Los Angeles County")
LA.plumbing.ts = ts(LA.data$Housing_Incomplete_Plumbing,start=2010,end=2019)

plot(LA.plumbing.ts) #not stationary
plot(acf(LA.plumbing.ts)) #Significant ACF at lag 1 
plot(pacf(LA.plumbing.ts)) #No significant PACFs

#Calculate spatial autocorrelation (DON'T INTERPRET BC ct_w IS WRONG)
moran.test(soa2010$Housing_Incomplete_Plumbing,ct_w) #-0.06 no significant spatial autocorrelation
```


Time series for each subindex panel form 

```{r}
#Averages 2010-2019
par(mfrow = c(3,4))
plot(scores.avg)
plot(edu9.avg)
plot(edu.avg)
plot(whitecollar.avg)
plot(unemployment.avg)
plot(income.avg)
plot(disparity.avg)
plot(belowpov.avg)
plot(house.avg)
plot(rent.avg)
plot(nophone.avg)
plot(noplumbing.avg)


#Weighted averages 2010-2019
par(mfrow = c(3,4))
plot(scores.wavg)
plot(edu9.wavg)
plot(edu.wavg)
plot(edu.wavg)
plot(whitecollar.wavg)
plot(income.wavg)
plot(disparity.wavg)
plot(belowpov.wavg)
plot(house.wavg)
plot(rent.wavg)
plot(nophone.wavg)
plot(noplumbing.wavg)

```


