---
title: "HUGE model selection"
author: "Jeffrey Wu"
date: "2023-06-27"
output: pdf_document
---

Experimenting with different HUGE model selection criterion: 

```{r}
#Simulate data with some sparsity structure
data = huge.generator(n=10000,d=5,graph="cluster")
plot(data)

#Grab sample covariance matrix and generate MVN data with it 
scov = data$sigmahat
adj = data$theta
adj = as.matrix(adj)

data2 = mvrnorm(n=1000,rep(0,5),scov)

#Estimate w HUGE (glasso with fewer observations)

#Use Sang's robust selection function to choose best regularization parameter for glasso
lambda = robsel(data2,alpha=0.95,B=200)

out = huge(data2,lambda = lambda,method="glasso")

#Select graphs w different selection criterion (RIC,EBIC,STARS)
out.ric = huge.select(out,criterion = "ric")
out.ebic = huge.select(out,criterion = "ebic")
out.stars = huge.select(out,criterion = "stars")

plot(out.ric)
plot(out.ebic)
plot(out.stars)

prec.ric = out.ric$opt.icov
prec.ebic = out.ebic$opt.icov
prec.stars = out.stars$opt.icov

#Create confusion matrix to see how different criterion perform against each other 
m1 = out.ric$refit
m2 = out.ebic$refit
m3 = out.stars$refit

# library(causalDisco)
# 
# confusion(m1,adj,type = "adj") #not giving out correct total number?
# confusion(m2,adj,type = "adj")
# confusion(m3,adj,type = "adj")

# adj58 = as(adj58,"CsparseMatrix")
# m1 = as(m1,"CsparseMatrix")
# 
# #Why doesn't huge.roc work on these 
# huge.roc(out.ebic$path,adj58)

library(caret)

confusionMatrix(factor(c(m1)),factor(c(adj)))
confusionMatrix(factor(c(m2)),factor(c(adj)))
confusionMatrix(factor(c(m3)),factor(c(adj)))
```

For 5 units (10 data points), 100 observations: PROB MOST SIMILAR TO DATA FOR PAPER 1

RIC: TP 4, TN 9, FP 8, FN 4
EBIC: TP 9, TN 8, FP 8, FN 0
STARS: TP 9, TN 8, FP 8, FN 0


For 5 units (100 data points), 1000 observations: 

RIC: TP 8, TN 17, FP 0, FN 0
EBIC: TP 8, TN 17, FP 0, FN 0
STARS: TP 8, TN 17, FP 0, FN 0


For 5 units (1000 data points), 10000 observations: 

RIC: TP 8, TN 13, FP 4, FN 0
EBIC: TP 8, TN 13, FP 4, FN 0
STARS: TP 8, TN 13, FP 4, FN 0

Note: Basically same results every time


For 10 units (10 data points), 20 observations: 

RIC: TP 14, TN 46, FP 14, FN 26
EBIC: TP 16, TN 42, FP 18, FN 24
STARS: TP 16, TN 42, FP 18, FN 24


For 10 units (100 data points), 200 observations: 

RIC: TP 26, TN 60, FP 0, FN 14
EBIC: TP 24, TN 60, FP 0, FN 16
STARS: TP 24, TN 60, FP 0, FN 16


For 10 units (1000 data points), 2000 observations: 

RIC: TP 40, TN 58, FP 2, FN 0
EBIC: TP 40, TN 58, FP 2, FN 0
STARS: TP 40, TN 58, FP 2, FN 0

Note: EBIC and STARS give same results every time

------------------------------------------------------------------------------

For 58 units (200 data points), 20 observations: 

RIC: TP 0, TN 3052, FP 0, FN 142
EBIC: TP 2, TN 3046, FP 6, FN 2
STARS: TP 2, TN 3046, FP 6, FN 2


For 58 units (2000 data points), 200 observations: 

RIC: TP 192, TN 2946, FP 84, FN 142
EBIC: TP 202, TN 2934, FP 96, FN 202
STARS: TP 202, TN 2934, FP 96, FN 202


For 58 units (20000 data points), 2000 observations: PROB MOST SIMILAR TO DATA FOR PAPER 2

RIC: TP 336, TN 2880, FP 148, FN 0
EBIC: TP 336, TN 2890, FP 138, FN 0
STARS: TP 336, TN 2890, FP 138, FN 0

Note: EBIC and STARS give same results every time (using both functions)


Downloading and formatting data: 

```{r}
library(dplyr) 
library(stringr)
library(zoo)
library(ggplot2)
library(urbnmapr)
library(devtools)
library(readxl)
library(spdep)
library(sp)
library(INLA)
library(HMMpa)
library(invgamma)
library(robsel)

soa.data = read_xlsx("SoA.data.1019.xlsx")

###county_flips are unique identifier for counties
soa.data$county_fips = as.character(soa.data$county_fips) ##change it to character

#Comparing distributions between 2010-2019
df = data.frame(rep(0,58))

for (i in 2010:2019){
  data = soa.data %>% filter(Year == i) %>% select(Score)
  df = cbind(df,data)
}
colnames(df) = c("a","2010","2011","2012","2013","2014","2015",
                 "2016","2017","2018","2019")
```


Generate more data 2 ways and compare huge estimations: 

```{r}
###STANDARD BOOTSTRAP (nonparametric)
score.data = df[,2:11]
sample.idx = round(runif(100,1,10))

score.boot = score.data[,sample.idx]
colnames(score.boot) = NULL
#score.boot = cbind(soa.data$county_name[1:58],score.boot)

#HUGE graph estimation of full adj matrix 
library(huge)

#Use Sang's robust selection function to choose best regularization parameter for glasso
lambda.boot = robsel(t(score.boot),alpha=0.95,B=200)

out.boot = huge(t(score.boot),lambda = lambda.boot,method="glasso")
#out.boot$path[[5]]
#out.boot$icov[[5]]

#est optimal adj matrix
boot.ric = huge.select(out.boot,criterion = "ric")
plot(boot.ric)

#est optimal precision matrix 
boot.ric$icov
#boot.ric$opt.icov[[1]]
#norm(boot.ric$opt.icov) #norm is about 0.5

###SKATER clustering -> HUGE graph estimation of full adj matrix 

###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

#I feel like it should be 58 rows, 100 columns since each column is treated as its own feature 
score_scaled = scale(score.boot)
covariates_scale = data.frame(score_scaled)

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")


#Get avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
CA_cluster = data.frame(CA_sf$NAMELSAD,clus5$groups)
names(CA_cluster) = c("County","Cluster")

CA_cluster = rbind(clus5$groups,t(score.boot))

cluster_features_boot = matrix(NA,nrow = 100,ncol = 5)

for (i in 1:5){
  idx = which(CA_cluster[1,] == i)
  
  for(j in 1:100){
  sum = sum(CA_cluster[j+1,idx])
  avg = sum / length(idx)
  cluster_features_boot[j,i] = avg
  }
}

#Graph learning w HUGE

#Use Sang's robust selection function to choose best regularization parameter for glasso
lambda.skboot = robsel(cluster_features_boot,alpha=0.95,B=200)

out.skboot = huge(cluster_features_boot,lambda=lambda.skboot,method="glasso")
# out.skboot$path[[5]]
# out.skboot$icov[[5]]

#est optimal adj matrix
skboot.stars = huge.select(out.skboot,criterion = "stars")
skboot.stars$refit
plot(skboot.stars)

skboot.stars$icov

#est optimal precision matrix 
skboot.stars$opt.icov #why is this null for ric method? 
norm(skboot.stars$opt.icov) #norm is about 0.978 for stars 
```


Only one edge missing (1 to 3)

Clusters breakdown: 1  2  3  4  5 
                    32  4  7 14  1 



```{r}
###GP KRIGING (parametric)
library(GauPro)
x = c(2010:2019)
xtest =seq(2010,2019,length=100)
preds.gp = matrix(NA,nrow = 100,ncol = 58)

# y = score.data[1,]
# score.gpmod = GauPro(x,as.numeric(y), kernel = kern.exp, parallel = FALSE)
# plot(x,y)
# curve(score.gpmod$predict(x),add = T, col = 2)
# preds = score.gpmod$predict(xtest)

kern.exp = Exponential$new(0)
kern.matern = Matern52$new(0)

for(i in 1:58){
  y = score.data[i,]
  score.gpmod = GauPro(x,as.numeric(y), kernel = kern.matern, parallel = FALSE)
  preds.gp[,i] = score.gpmod$predict(xtest)
}

#Got errors/warnings with the fit but looks fine for random counties 
a = round(runif(1,1,58))
plot(x,score.data[a,])
plot(xtest,preds.gp[,a])

#HUGE graph estimation of full adj matrix 

#Use Sang's robust selection function to choose best regularization parameter for glasso
lambda.gp = robsel(preds.gp,alpha=0.95,B=200)

out.gp = huge(preds.gp,lambda=lambda.gp,method="glasso")
# out.gp$path[[5]]
# out.gp$icov[[5]]


#est optimal adj matrix
gp.ebic = huge.select(out.gp,criterion = "ebic")
gp.ebic$refit
plot(gp.ebic)

#est optimal precision matrix 
gp.ebic$opt.icov #why is this null for ric method???
#norm(gp.ric$opt.icov) #norm is 0.5 for stars


###SKATER clustering -> HUGE graph estimation of full adj matrix 
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

score_scaled = scale(preds.gp)
covariates_scale = data.frame(t(score_scaled))

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster example")


#HUGE estimation on clustered groups 
#Get avg value for Score for each cluster for each year 
#Create new data matrix of aggregated feature vectors 
CA_cluster = rbind(clus5$groups,preds.gp)

cluster_features_gp = matrix(NA,nrow = 100,ncol = 5)

for (i in 1:5){
  idx = which(CA_cluster[1,] == i)
  
  for(j in 1:100){
  sum = sum(CA_cluster[j+1,idx])
  avg = sum / length(idx)
  cluster_features_gp[j,i] = avg
  }
}

#Use Sang's robust selection function to choose best regularization parameter for glasso
lambda.skgp = robsel(cluster_features_gp,alpha=0.95,B=200)

#Graph learning w HUGE
out.skgp = huge(cluster_features_gp,lambda=lambda.skgp,method="glasso",cov.output = T)
# out.skgp$path[[5]]
# out.skgp$icov[[5]]


#est optimal adj matrix
skgp.stars = huge.select(out.skgp,criterion = "stars")
skgp.stars$refit
plot(skgp.stars)

#est optimal precision matrix 
skgp.stars$opt.icov #why is this null for ric method???
norm(skgp.stars$opt.icov) #norm is 0.5016994 for stars
```

HUGE estimates completely connected graph 

SKATER clustering under GP kriging: 1  2  3  4  5 
                                    33  3  9  9  4 



Hypothesis testing for 2 covariance matrices (from Cai 2013 paper)

H0: 2 sample covariances are equal

Function implementing 2 equal covariance matrix hypothesis testing: 

```{r}
twocovtest = function(data1,data2,alpha){
  p = ncol(data1)
  n = nrow(data1)
  data1 = scale(data1)
  data2 = scale(data2)
  
  xbars = apply(data1,2,mean)
  ybars = apply(data2,2,mean)
  scov1 = matrix(NA,p,p)
  scov2 = matrix(NA,p,p)

  #calculating covariances between each cluster group
  for (i in 1:p){
    for (j in i:p){
      #calculating covariances between each cluster group
      scov1[i,j] = t(data1[,i] - xbars[i]) %*% (data1[,j] - xbars[j]) / n
      scov1[j,i] = scov1[i,j]
      
      scov2[i,j] = t(data2[,i] - ybars[i]) %*% (data2[,j] - ybars[j]) / n
      scov2[j,i] = scov2[i,j]
    }
  }
  
  #calculate corresponding thetas
  theta1hat = matrix(NA,p,p)
  theta2hat = matrix(NA,p,p)
  
  for (i in 1:p){
    for (j in i:p){
      #calculating covariances between each cluster group
      theta1hat[i,j] = sum((((data1[,i] - xbars[i])*(data1[,j] - xbars[j])) - scov1[i,j])^2) / n
      
      theta2hat[i,j] = sum((((data2[,i] - xbars[i])*(data2[,j] - xbars[j])) - scov2[i,j])^2) / n
      
      theta1hat[j,i] = theta1hat[i,j]
      theta2hat[j,i] = theta2hat[i,j]
    }
  }
  
  #calculate M statistics
  W = matrix(NA,p,p)
  W = (scov1 - scov2) / sqrt((theta1hat/n) + (theta2hat/n))
  
  M = W^2
  TS = max(M)
  
  #use extreme value distribution  
  qalpha = -log(8*pi) - 2*log(log((1-alpha)^-1))
  CV = qalpha + (4*log(p)) - (log(log(p)))
  
  #reject hypothesis or not? REJECT IF TRUE 
  return(c(TS,CV))
  #return(CV)
  #return((TS > CV) || (TS == CV))
}

twocovtest(t(score.boot),preds.gp,0.05)
twocovtest(cluster_features_boot,cluster_features_gp,0.05)
```

For non clustered data: 

Fail to reject H0 so we conclude that the 2 resampling methods resulted in HUGE estimated graphs that have similar (not significantly significantly different) sample covariance matrices

For clustered data: 

Reject H0 so we conclude that the 2 resampling methods resulted in HUGE estimated graphs (on the clustered features) that have statistically significantly different sample covariance matrices




(Hardcoded version)

```{r}
###Steps 

#obtain sample covariance matrices 
p = nrow(Sigmahat.skboot)
n = 100
head(Sigmahat.skboot)
head(Sigmahat.skgp)

cluster_features_boot = scale(cluster_features_boot)
cluster_features_gp = scale(cluster_features_gp)

xbars = apply(cluster_features_boot,2,mean)
ybars = apply(cluster_features_gp,2,mean)
scov.boot = matrix(NA,5,5)
scov.gp = matrix(NA,5,5)

#calculating covariances between each cluster group
for (i in 1:p){
  for (j in i:p){
    scov.boot[i,j] = t(cluster_features_boot[,i] - xbars[i]) %*% (cluster_features_boot[,j] - xbars[j]) / n
    scov.boot[j,i] = scov.boot[i,j]
    
    scov.gp[i,j] = t(cluster_features_gp[,i] - ybars[i]) %*% (cluster_features_gp[,j] - ybars[j]) / n
    scov.gp[j,i] = scov.gp[i,j]
  }
}

#calculate corresponding thetas
theta1hat = matrix(NA,5,5)
theta2hat = matrix(NA,5,5)

for (i in 1:p){
  for (j in i:p){
    #calculating covariances between each cluster group
    theta1hat[i,j] = sum((((cluster_features_boot[,i] - xbars[i])*(cluster_features_boot[,j] - xbars[j])) - scov.boot[i,j])^2) / n
    
    theta2hat[i,j] = sum((((cluster_features_gp[,i] - xbars[i])*(cluster_features_gp[,j] - xbars[j])) - scov.gp[i,j])^2) / n
    
    theta1hat[j,i] = theta1hat[i,j]
    theta2hat[j,i] = theta2hat[i,j]
  }
}

#calculate M statistics
W = matrix(NA,5,5)

W = (scov.boot - scov.gp) / sqrt((theta1hat/n) + (theta2hat/n))

M = W^2
TS = max(M)

#use extreme value distribution  
alpha = 0.05
qalpha = -log(8*pi) - 2*log(log((1-alpha)^-1))
CV = qalpha + (4*log(p)) - (log(log(p)))

#reject hypothesis or not? 
(TS > CV) || (TS == CV)
```

