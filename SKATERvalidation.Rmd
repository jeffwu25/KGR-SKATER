---
title: "SKATER validation"
author: "Jeffrey Wu"
date: "2023-05-23"
output: pdf_document
---

Starting toy data: 

```{r}
library(MASS)

x1 = CA_newdata$EDUC_college
x2 = CA_newdata$Unemployment_Rate
N = length(x1)

sigma = 1
alpha = 0.5

Ksigma = matrix(data = NA,nrow = N, ncol = N)

for (i in 1:N){
  for (j in 1:N){
    Ksigma[i,j] = exp(-((x1[i]-x1[j])^2 + (x2[i]-x2[j])^2) / (2*sigma^2))
    }
}

#Set mean 0 generally
lambda0 = 1
mu0 = 1

#Generate toy hospitalization counts (time series of 100 for each county )
Hospitalizations = matrix(0,nrow = 100,ncol = 58)

for (i in 1:100){
  lambdas = lambda0*mu0*exp(mvrnorm(n=1,rep(0,N),Ksigma))
  for (j in 1:N){
    Hospitalizations[i,j] = rgenpois(1,lambda1 = lambdas[j],lambda2 = alpha)
  }
}

hosp_standardized = Hospitalizations

for (i in 1:58){
  means = colMeans(Hospitalizations)
  hosp_standardized[,i] = hosp_standardized[,i] - means[i]
}


#Hospitalizations = data.frame(Hospitalizations)
#toydata1 = cbind(CA_newdata,Hospitalizations)

#Calculate sample covariance matrix (Sigma1) -> make sure it's positive def
Sigma1 = (t(hosp_standardized) %*% hosp_standardized) / 99

library(reshape2)
melted_cormat <- melt(Sigma1)
head(melted_cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```



```{r}
#Generate random graph w cluster structure
clustered58 = huge.generator(n=100,d=58,graph = "cluster",g=5)
plot(clustered58)

#Grab adjacency matrix 
adj58 = clustered58$theta
adj58 = as.matrix(adj58)

#Simplify clustering
#Create new adj matrix w no edges between clusters 1 and 5 

# adj58v2 = matrix(1,nrow=58,ncol=58)
# for (i in 1:1){
#   for (j in 58:58){
#     adj58v2[i,j] = 0 
#     adj58v2[j,i] = 0
#   }
# }
# 
# for(i in 1:58){
#   adj58[i,i] = 1
# }

View(as.matrix(adj58))

adj59 = adj58 + diag(58)

library(reshape2)
melted_cormat <- melt(adj59)
head(melted_cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

#Multiply to PRECISION MATRIX to induce sparseness / clustered structure 
prec1 = solve(Sigma1)

library(reshape2)
melted_cormat <- melt(prec1)
head(melted_cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()


prec2 = adj59 * prec1 


library(reshape2)
melted_cormat <- melt(prec2)
head(melted_cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

diag(prec2)


# prec2 = prec2 + diag(58)

Sigma2 = solve(prec2)



library(reshape2)
melted_cormat <- melt(Sigma2)
head(melted_cormat)

library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

det(Sigma2)


subblock1 = Sigma2[1:11,1:11]

#What should I do if this matrix is not positive definite??? This inflates variance a lot 
#Sigma2 = (t(Sigma2) %*% Sigma2) + diag(58)
```

Generate new toy data with Sigma2

```{r}
sigma = 1
alpha = 0.5

#Covariance eq from graph regression paper? 
#Sigma = kronecker(Ksigma,adj^2)

#Set mean 0 generally
lambda0 = 1
mu0 = 1

lambdas = lambda0*mu0*exp(mvrnorm(n=1,rep(0,58),Sigma2))
Hospitalizations = rep(0,58)

for (i in 1:N){
  Hospitalizations[i] = rgenpois(1,lambda1 = lambdas[j],lambda2 = alpha)
}

# Hospitalizations = matrix(0,nrow = 100,ncol = 58)
# 
# for (i in 1:100){
#   lambdas = lambda0*mu0*exp(mvrnorm(n=1,rep(0,N),Ksigma))
#   for (j in 1:N){
#     Hospitalizations[i,j] = rgenpois(1,lambda1 = lambdas[j],lambda2 = alpha)
#   }
# }
```



Run SKATER on newly generated hospitalizations data: 

SKATER clustering: 

```{r}
###Setting up SPDF for CA counties 
CA_sf = st_read(getwd(),"CA_Counties_TIGER2016")
CA_spdf = as_Spatial(CA_sf)

hosp_scaled = scale(Hospitalizations)
covariates_scale = data.frame(hosp_scaled)

CA_spdf@data = covariates_scale

#Identify neighborhood list for counties 
CA_nb = poly2nb(CA_spdf)

# plot(CA_spdf, main = "With queen")
# plot(CA_nb, coords = coordinates(CA_spdf), col="blue", add = TRUE)

#Calculate edge costs (dissimilarity matrix) based on Euclidean distance 
costs <- nbcosts(CA_nb, data = covariates_scale)

###Get adjacency matrix using nb2mat() (SEPARATE STEP FOR INLA)
#adj = nb2mat(CA_nb,style = "B")

#Transform edge costs to spatial weights 
ct_w <- nb2listw(CA_nb,costs,style="B")

#Create minimum spanning tree 
ct_mst <- mstree(ct_w)

#Run SKATER algorithm to get 5 contiguous clusters (cluster idx is in order of CA_sf)
clus5 <- skater(edges = ct_mst[,1:2], data = covariates_scale, ncuts = 4)

#Plot clustered CA
plot((CA_sf %>% mutate(clus = clus5$groups))['clus'], main = "5 cluster from SKATER")

clus5groups = clus5$groups

#Hardcode covariance matrix 
Sigma3 = matrix(NA,nrow = 58,ncol=58)
for (i in 1:58){
  for (j in 1:58){
    if (i == j){
      Sigma3[i,j] = 1
    }
    
    else if (clus5groups[i] != clus5groups[j]){
      Sigma3[i,j] = 0
    }
    
    else if(clus5groups[i] == clus5groups[j]){
      Sigma3[i,j] = 0.5
    }
  }
}

head(Sigma3)  
```



